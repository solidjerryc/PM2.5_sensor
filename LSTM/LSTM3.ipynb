{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17084, 100, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(17084, 60)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense,Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import CuDNNLSTM\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data=pd.read_csv(r\"data4.csv\",index_col='no')\n",
    "\n",
    "internal=100\n",
    "\n",
    "scalerx = preprocessing.MinMaxScaler().fit(data.iloc[:,1:])\n",
    "data1=scalerx.transform(data.iloc[:,1:])\n",
    "\n",
    "x=[]\n",
    "y_former=[]\n",
    "y=[]\n",
    "\n",
    "for i in range(data1.shape[0]-internal-61):\n",
    "    x.append(np.array(data1[i:i+internal]))\n",
    "    y.append(np.array(data.iloc[i+internal+1:i+internal+61,1]))\n",
    "\n",
    "x=np.array(x)\n",
    "#y_former=np.array(y_former).reshape((-1,1))\n",
    "y=np.array(y).reshape((-1,60))\n",
    "\n",
    "# 归一化y\n",
    "scalery = preprocessing.MinMaxScaler().fit(y)\n",
    "y=scalery.transform(y)\n",
    "#y_former=scalery.transform(y_former)\n",
    "\n",
    "x_train,y_train=x[:15000],y[:15000]\n",
    "x_test,y_test=x[15000:],y[15000:]\n",
    "\n",
    "print(x.shape)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12000 samples, validate on 3000 samples\n",
      "Epoch 1/10\n",
      "12000/12000 [==============================] - 31s 3ms/step - loss: 0.0041 - acc: 0.0200 - val_loss: 0.0011 - val_acc: 0.0157\n",
      "Epoch 2/10\n",
      "12000/12000 [==============================] - 26s 2ms/step - loss: 0.0028 - acc: 0.0241 - val_loss: 0.0016 - val_acc: 0.0247\n",
      "Epoch 3/10\n",
      "12000/12000 [==============================] - 26s 2ms/step - loss: 0.0027 - acc: 0.0248 - val_loss: 0.0012 - val_acc: 0.0383\n",
      "Epoch 4/10\n",
      "12000/12000 [==============================] - 26s 2ms/step - loss: 0.0025 - acc: 0.0303 - val_loss: 0.0010 - val_acc: 0.0200\n",
      "Epoch 5/10\n",
      "12000/12000 [==============================] - 26s 2ms/step - loss: 0.0024 - acc: 0.0313 - val_loss: 0.0011 - val_acc: 0.0223\n",
      "Epoch 6/10\n",
      "12000/12000 [==============================] - 27s 2ms/step - loss: 0.0023 - acc: 0.0335 - val_loss: 0.0013 - val_acc: 0.0220\n",
      "Epoch 7/10\n",
      "12000/12000 [==============================] - 27s 2ms/step - loss: 0.0022 - acc: 0.0357 - val_loss: 0.0012 - val_acc: 0.0203\n",
      "Epoch 8/10\n",
      "12000/12000 [==============================] - 26s 2ms/step - loss: 0.0020 - acc: 0.0368 - val_loss: 0.0022 - val_acc: 0.0317\n",
      "Epoch 9/10\n",
      "12000/12000 [==============================] - 27s 2ms/step - loss: 0.0019 - acc: 0.0388 - val_loss: 0.0011 - val_acc: 0.0170\n",
      "Epoch 10/10\n",
      "12000/12000 [==============================] - 27s 2ms/step - loss: 0.0019 - acc: 0.0363 - val_loss: 0.0015 - val_acc: 0.0267\n",
      "2084/2084 [==============================] - 7s 4ms/step\n",
      "15000/15000 [==============================] - 54s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(CuDNNLSTM(32,input_shape=(x_train.shape[1], x_train.shape[2])))#, init='uniform'\n",
    "\n",
    "model.add(Dense(60))\n",
    "model.add(Activation('sigmoid'))\n",
    "adam = Adam(lr=0.01)\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=adam, metrics=[\"accuracy\"])\n",
    "\n",
    "hist = model.fit(x_train, y_train, batch_size=5, epochs=10, shuffle=True,verbose=1,validation_split=0.2)\n",
    "\n",
    "\n",
    "y_predict = model.predict(x_test, batch_size=1, verbose=1)\n",
    "\n",
    "y_predict = scalery.inverse_transform(y_predict)\n",
    "y_test_yuan = scalery.inverse_transform(y_test)\n",
    "\n",
    "\n",
    "y_train_predict = model.predict(x_train, batch_size=1, verbose=1)\n",
    "\n",
    "y_train_predict = scalery.inverse_transform(y_train_predict)\n",
    "y_train_yuan = scalery.inverse_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2084, 60)\n",
      "(2084, 60)\n",
      "(15000, 60)\n",
      "(15000, 60)\n"
     ]
    }
   ],
   "source": [
    "print(y_predict.shape)\n",
    "print(y_test_yuan.shape)\n",
    "\n",
    "print(y_train_yuan.shape)\n",
    "print(y_train_predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([28.61728 , 28.619196, 28.769655, ..., 37.258167, 37.427547,\n",
       "       37.290665], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict.reshape((-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.3152831379899474\n",
      "5.0017975998625985\n"
     ]
    }
   ],
   "source": [
    "print(np.sqrt(sum((y_predict.reshape((-1,))-y_test_yuan.reshape((-1,)))**2)/len(y_predict.reshape((-1,)))))\n",
    "\n",
    "print(np.sqrt(sum((y_train_yuan.reshape((-1,))-y_train_predict.reshape((-1,)))**2)/len(y_train_yuan.reshape((-1,)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM 循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12000 samples, validate on 3000 samples\n",
      "Epoch 1/20\n",
      "12000/12000 [==============================] - 42s 3ms/step - loss: 0.0024 - acc: 0.0386 - val_loss: 0.0016 - val_acc: 0.0307\n",
      "Epoch 2/20\n",
      "12000/12000 [==============================] - 40s 3ms/step - loss: 0.0018 - acc: 0.0435 - val_loss: 0.0010 - val_acc: 0.0283\n",
      "Epoch 3/20\n",
      "12000/12000 [==============================] - 40s 3ms/step - loss: 0.0017 - acc: 0.0423 - val_loss: 9.7933e-04 - val_acc: 0.0360\n",
      "Epoch 4/20\n",
      "12000/12000 [==============================] - 40s 3ms/step - loss: 0.0017 - acc: 0.0460 - val_loss: 9.8070e-04 - val_acc: 0.0560\n",
      "Epoch 5/20\n",
      "12000/12000 [==============================] - 39s 3ms/step - loss: 0.0016 - acc: 0.0450 - val_loss: 9.8775e-04 - val_acc: 0.0333\n",
      "Epoch 6/20\n",
      "12000/12000 [==============================] - 39s 3ms/step - loss: 0.0016 - acc: 0.0479 - val_loss: 9.8414e-04 - val_acc: 0.0413\n",
      "Epoch 7/20\n",
      "12000/12000 [==============================] - 39s 3ms/step - loss: 0.0015 - acc: 0.0500 - val_loss: 0.0011 - val_acc: 0.0457\n",
      "Epoch 8/20\n",
      "12000/12000 [==============================] - 39s 3ms/step - loss: 0.0015 - acc: 0.0455 - val_loss: 0.0011 - val_acc: 0.0420\n",
      "Epoch 9/20\n",
      "12000/12000 [==============================] - 40s 3ms/step - loss: 0.0015 - acc: 0.0463 - val_loss: 0.0012 - val_acc: 0.0397\n",
      "Epoch 10/20\n",
      "12000/12000 [==============================] - 40s 3ms/step - loss: 0.0014 - acc: 0.0496 - val_loss: 0.0012 - val_acc: 0.0433\n",
      "Epoch 11/20\n",
      "12000/12000 [==============================] - 41s 3ms/step - loss: 0.0014 - acc: 0.0496 - val_loss: 0.0012 - val_acc: 0.0527\n",
      "Epoch 12/20\n",
      "12000/12000 [==============================] - 41s 3ms/step - loss: 0.0014 - acc: 0.0506 - val_loss: 0.0012 - val_acc: 0.0440\n",
      "Epoch 13/20\n",
      "12000/12000 [==============================] - 41s 3ms/step - loss: 0.0013 - acc: 0.0531 - val_loss: 0.0011 - val_acc: 0.0460\n",
      "Epoch 14/20\n",
      "12000/12000 [==============================] - 41s 3ms/step - loss: 0.0020 - acc: 0.0518 - val_loss: 0.0018 - val_acc: 0.0463\n",
      "Epoch 15/20\n",
      "12000/12000 [==============================] - 41s 3ms/step - loss: 0.0016 - acc: 0.0448 - val_loss: 0.0012 - val_acc: 0.0340\n",
      "Epoch 16/20\n",
      "12000/12000 [==============================] - 41s 3ms/step - loss: 0.0014 - acc: 0.0481 - val_loss: 0.0012 - val_acc: 0.0433\n",
      "Epoch 17/20\n",
      "12000/12000 [==============================] - 41s 3ms/step - loss: 0.0015 - acc: 0.0483 - val_loss: 0.0019 - val_acc: 0.0297\n",
      "Epoch 18/20\n",
      "12000/12000 [==============================] - 42s 3ms/step - loss: 0.0023 - acc: 0.0463 - val_loss: 0.0012 - val_acc: 0.0463\n",
      "Epoch 19/20\n",
      "12000/12000 [==============================] - 41s 3ms/step - loss: 0.0017 - acc: 0.0470 - val_loss: 0.0011 - val_acc: 0.0297\n",
      "Epoch 20/20\n",
      "12000/12000 [==============================] - 43s 4ms/step - loss: 0.0015 - acc: 0.0424 - val_loss: 0.0011 - val_acc: 0.0380\n",
      "1974/1974 [==============================] - 12s 6ms/step\n",
      "15000/15000 [==============================] - 93s 6ms/step\n",
      "[240, 0.01, 4.1180641534045614, 4.49163394522702]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense,Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import CuDNNLSTM\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data=pd.read_csv(r\"data4.csv\",index_col='no')\n",
    "\n",
    "#internal=100\n",
    "internals=[240]\n",
    "lrs=[0.01]\n",
    "\n",
    "scalerx = preprocessing.MinMaxScaler().fit(data.iloc[:,1:])\n",
    "data1=scalerx.transform(data.iloc[:,1:])\n",
    "\n",
    "rmse_train=[]\n",
    "rmse_test=[]\n",
    "\n",
    "for internal in internals:\n",
    "    for lr in lrs:\n",
    "\n",
    "        x=[]\n",
    "        y_former=[]\n",
    "        y=[]\n",
    "\n",
    "        for i in range(data1.shape[0]-internal-31):\n",
    "            x.append(np.array(data1[i:i+internal]))\n",
    "            y.append(np.array(data.iloc[i+internal+1:i+internal+31,1]))\n",
    "\n",
    "        x=np.array(x)\n",
    "        #y_former=np.array(y_former).reshape((-1,1))\n",
    "        y=np.array(y).reshape((-1,30))\n",
    "\n",
    "        # 归一化y\n",
    "        scalery = preprocessing.MinMaxScaler().fit(y)\n",
    "        y=scalery.transform(y)\n",
    "        #y_former=scalery.transform(y_former)\n",
    "\n",
    "        x_train,y_train=x[:15000],y[:15000]\n",
    "        x_test,y_test=x[15000:],y[15000:]\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(CuDNNLSTM(32,input_shape=(x_train.shape[1], x_train.shape[2])))#, init='uniform'\n",
    "\n",
    "        model.add(Dense(30))\n",
    "        model.add(Activation('sigmoid'))\n",
    "        adam = Adam(lr=lr)\n",
    "\n",
    "        model.compile(loss='mean_squared_error', optimizer=adam, metrics=[\"accuracy\"])\n",
    "\n",
    "        hist = model.fit(x_train, y_train, batch_size=5, epochs=20, shuffle=True,verbose=1,validation_split=0.2)\n",
    "\n",
    "\n",
    "        y_predict = model.predict(x_test, batch_size=1, verbose=1)\n",
    "\n",
    "        y_predict = scalery.inverse_transform(y_predict)\n",
    "        y_test_yuan = scalery.inverse_transform(y_test)\n",
    "\n",
    "\n",
    "        y_train_predict = model.predict(x_train, batch_size=1, verbose=1)\n",
    "\n",
    "        y_train_predict = scalery.inverse_transform(y_train_predict)\n",
    "        y_train_yuan = scalery.inverse_transform(y_train)\n",
    "\n",
    "        rmse1=np.sqrt(sum((y_predict.reshape((-1,))-y_test_yuan.reshape((-1,)))**2)/len(y_predict.reshape((-1,))))\n",
    "        rmse2=np.sqrt(sum((y_train_yuan.reshape((-1,))-y_train_predict.reshape((-1,)))**2)/len(y_train_yuan.reshape((-1,))))\n",
    "        \n",
    "        print([internal,lr,rmse1,rmse2])\n",
    "        rmse_test.append(rmse1)\n",
    "        rmse_train.append(rmse2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.49163394522702]\n",
      "[4.1180641534045614]\n"
     ]
    }
   ],
   "source": [
    "print(rmse_train)\n",
    "print(rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"data/y_test.npy\",y_test_yuan)\n",
    "np.save(\"data/y_test_pre.npy\",y_predict)\n",
    "np.save(\"data/y_train.npy\",y_train_yuan)\n",
    "np.save(\"data/y_train_pre.npy\",y_train_predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('LSTM_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12000 samples, validate on 3000 samples\n",
      "Epoch 1/20\n",
      "  615/12000 [>.............................] - ETA: 14:07 - loss: 0.0169 - acc: 0.0325"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-9902b5b7aaa1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean_squared_error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    963\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 965\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    966\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1669\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1204\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1206\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1207\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1208\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2475\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2476\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense,Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data=pd.read_csv(r\"data4.csv\",index_col='no')\n",
    "\n",
    "#internal=100\n",
    "internals=[240]\n",
    "lrs=[0.01]\n",
    "\n",
    "scalerx = preprocessing.MinMaxScaler().fit(data.iloc[:,1:])\n",
    "data1=scalerx.transform(data.iloc[:,1:])\n",
    "\n",
    "rmse_train=[]\n",
    "rmse_test=[]\n",
    "\n",
    "for internal in internals:\n",
    "    for lr in lrs:\n",
    "\n",
    "        x=[]\n",
    "        y_former=[]\n",
    "        y=[]\n",
    "\n",
    "        for i in range(data1.shape[0]-internal-31):\n",
    "            x.append(np.array(data1[i:i+internal]))\n",
    "            y.append(np.array(data.iloc[i+internal+1:i+internal+31,1]))\n",
    "\n",
    "        x=np.array(x)\n",
    "        #y_former=np.array(y_former).reshape((-1,1))\n",
    "        y=np.array(y).reshape((-1,30))\n",
    "\n",
    "        # 归一化y\n",
    "        scalery = preprocessing.MinMaxScaler().fit(y)\n",
    "        y=scalery.transform(y)\n",
    "        #y_former=scalery.transform(y_former)\n",
    "\n",
    "        x_train,y_train=x[:15000],y[:15000]\n",
    "        x_test,y_test=x[15000:],y[15000:]\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(LSTM(32,input_shape=(x_train.shape[1], x_train.shape[2])))#, init='uniform'\n",
    "\n",
    "        model.add(Dense(30))\n",
    "        model.add(Activation('sigmoid'))\n",
    "        adam = Adam(lr=lr)\n",
    "\n",
    "        model.compile(loss='mean_squared_error', optimizer=adam, metrics=[\"accuracy\"])\n",
    "\n",
    "        hist = model.fit(x_train, y_train, batch_size=5, epochs=20, shuffle=True,verbose=1,validation_split=0.2)\n",
    "\n",
    "\n",
    "        y_predict = model.predict(x_test, batch_size=1, verbose=1)\n",
    "\n",
    "        y_predict = scalery.inverse_transform(y_predict)\n",
    "        y_test_yuan = scalery.inverse_transform(y_test)\n",
    "\n",
    "\n",
    "        y_train_predict = model.predict(x_train, batch_size=1, verbose=1)\n",
    "\n",
    "        y_train_predict = scalery.inverse_transform(y_train_predict)\n",
    "        y_train_yuan = scalery.inverse_transform(y_train)\n",
    "\n",
    "        rmse1=np.sqrt(sum((y_predict.reshape((-1,))-y_test_yuan.reshape((-1,)))**2)/len(y_predict.reshape((-1,))))\n",
    "        rmse2=np.sqrt(sum((y_train_yuan.reshape((-1,))-y_train_predict.reshape((-1,)))**2)/len(y_train_yuan.reshape((-1,))))\n",
    "        \n",
    "        print([internal,lr,rmse1,rmse2])\n",
    "        rmse_test.append(rmse1)\n",
    "        rmse_train.append(rmse2)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
